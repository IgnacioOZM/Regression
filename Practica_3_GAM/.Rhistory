PlotDataframe(fdata,output.name = "TMIN230")
## Filter outliers
fdata$TMAX417[fdata$TMAX417>35] = NA
#relation betwwen X and Y
PlotDataframe(fdata,output.name = "TMIN230")
library(MLTools)
## LoadData
fdataINI <- read.csv("TemperaturesSpainSmall.csv", sep = ";")
fdataTOT <- fdataINI[1:1000,] #Select 1000 for class purposes
fdataTOT = na.omit(fdataTOT) #Eliminate NA
# Contents of the dataset
str(fdataTOT)
# Output variable (TMIN MADRID-RETIRO)
summary(fdataTOT$TMIN230)
hist(fdataTOT$TMIN230, nclass = 40)
# Subsets variables
fdataTmax = select(fdataTOT, "WEEKDAY","MONTH", starts_with("TMAX"))
str(fdataTmax)
fdata = select(fdataTOT, "WEEKDAY","MONTH","TMIN230","TMAX230","TMAX229","TMAX237",
"TMAX417","TMAX2969","TMAX3910","TMAX3918","TMAX3959")
str(fdata)
## Exploratory analysis -------------------------------------------------------------------------------------
#correlation plot of numeric variables
numvars <- sapply(fdata, class) %in% c("integer","numeric")
C <- cor(fdata[,numvars])
corrplot(C, method = "circle")
#relation betwwen X and Y
PlotDataframe(fdata,output.name = "TMIN230")
## Filter outliers
fdata$TMAX417[fdata$TMAX417>35] = NA
fdata = na.omit(fdata) #Eliminate NA, i.e. the outliers
#Convert month to factor
fdata$MONTH <- as.factor(fdata$MONTH)
## Divide the data into training and validation sets ---------------------------------------------------
set.seed(150) #For replication
#create random 80/20 % split
trainIndex <- createDataPartition(fdata$TMIN230,      #output variable. createDataPartition creates proportional partitions
p = 0.8,      #split probability for training
list = FALSE, #Avoid output as a list
times = 1)    #only one partition
#obtain training and validation sets
fTR <- fdata[trainIndex,]
fTR_eval <- fTR #
fTS <- fdata[-trainIndex,]
## Initialize trainControl (FOR ALL MODELS) -----------------------------------------------------------------------
#Use resampling for measuring generalization error
#K-fold with 10 folds
ctrl_tune <- trainControl(method = "cv",
number = 10,
summaryFunction = defaultSummary,    #Performance summary for comparing models in hold-out samples.
returnResamp = "final",              #Return final information about resampling
savePredictions = TRUE)              #save predictions
#==================================================================
## FITH model for 230	MADRID - RETIRO (Collinearity ??)
# 229	  BADAJOZ/TALAVERA LA REAL
# 237	  VALENCIA
# 417	  GRANADA
# 2969	BARCELONA/AEROPUERTO
# 3910	GIJON
# 3918	PALMA DE MALLORCA CMT
# 3959	STA. CRUZ DE TENERIFE
#==================================================================
set.seed(150) #For replication
lm5.fit = train(form = TMIN230 ~ .,
data = fTR,
method = "lm", #Linear model
#tuneGrid = data.frame(intercept = TRUE),
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
lm5.fit #information about the resampling settings
summary(lm5.fit)  #information about the model trained
# pvalues suggest removing ...
#Identify correlated variables
vif(lm5.fit$finalModel)
#==================================================================
## SIXTH model for 230	MADRID - RETIRO (Removing correlated and irrelevant by hand)
#==================================================================
set.seed(150) #For replication
lm6.fit = train(form = TMIN230 ~ TMAX230 + TMAX237 + TMAX3910 + TMAX3918 + TMAX3959,
data = fTR,
method = "lm", #Linear model
#tuneGrid = data.frame(intercept = TRUE),
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
lm6.fit #information about the resampling settings
summary(lm6.fit)  #information about the model trained
# pvalues suggest removing ...
#Identify correlated variables
vif(lm6.fit$finalModel)
#==================================================================
## SEVENTH model for 230	MADRID - RETIRO (Removing irrelevant + collinearity automatically)
#==================================================================
## As there is no tuning parameter for linear regression, Cross-Validation is not necessary for variable selection.
## For other methods, the trainControl specified before can be used
set.seed(150) #For replication
## The control function is similar to trainControl()
ctrl_none <- trainControl(method = "none",
summaryFunction = defaultSummary,    #Performance summary for comparing models in hold-out samples.
returnResamp = "final",              #Return final information about resampling
savePredictions = TRUE)              #save predictions
## Specifies the cross validation method used for selecting the optimum number of variables
ctrl_rfe <- rfeControl(method = "cv",
number = 10,
verbose = TRUE,
functions = caretFuncs)
## rfe() function instead of train
set.seed(150)
subsets <- 1:20 #Grid for the number of features that should be retained (all, >7)
lm7.RFE <- rfe(form = TMIN230 ~ .,
data = fTR,
# Arguments passed to train() function
method = "lm",
preProcess = c("center","scale"),
trControl = ctrl_none,
# Arguments for rfe
sizes = subsets,
metric = "RMSE",
rfeControl = ctrl_rfe)
lm7.RFE # Cross validation results and variable selection
ggplot(lm7.RFE,metric = "RMSE")
lm7.RFE$fit #Final caret train() object
lm7.RFE$fit$finalModel #Final model trained
#si queremos solo las tres mejores variables
caretFuncs$selectVar( lm7.RFE$variables, size = 3)
vif(lm7.RFE$fit$finalModel)
#==================================================================
## EIGHTH model for TMIN230 USING RIDGE REGULARIZATION
#==================================================================
set.seed(150) #For replication
#With categorical variables, formula method should be used
lm8.fit = train(form = TMIN230 ~ .,
data = fTR,
method = "glmnet",
nlambda = 500, # Number of lambda values for glmnet function
tuneGrid = expand.grid(
lambda = 2*10^seq(-2,2, length =20),
alpha = 0),  # <- RIDGE
#tuneGrid = data.frame( lambda = 1,  alpha = 0), #Selecting one lambda value
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
lm8.fit #information about the resampling
ggplot(lm8.fit)+scale_x_log10()
coef(lm8.fit$finalModel)  #information about the model trained
#Plot the evolution of the coefficients as a function of lambda
plot(lm8.fit$finalModel, xvar = "lambda")
#Coefs for high value of lambda
lm8.fit$finalModel$lambda[3]
coef(lm8.fit$finalModel)[,3]
#Coefs for low value of lambda
lm8.fit$finalModel$lambda[500]
coef(lm8.fit$finalModel)[,500]
#==================================================================
## EIGHTH model for TMIN230 USING LASSO REGULARIZATION
#==================================================================
set.seed(150) #For replication
lm9.fit = train(form = TMIN230 ~ .,
data = fTR,
method = "glmnet",
nlambda = 500, # Number of lambda values for glmnet function
tuneGrid = expand.grid(
lambda = 2*10^seq(-2,0, length =20),
alpha = 1),
#tuneGrid = data.frame( lambda = 0.1,  alpha = 1), #Selecting one lambda value
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
lm9.fit #information about the resampling
ggplot(lm9.fit)+scale_x_log10()
coef(lm9.fit$finalModel)  #information about the model trained
#Plot the evolution of the coefficients as a function of lambda
plot(lm9.fit$finalModel, xvar = "lambda")
#Coefs for high value of lambda
lm9.fit$finalModel$lambda[3]
coef(lm9.fit$finalModel)[,3]
#Coefs for low value of lambda
lm9.fit$finalModel$lambda[300]
coef(lm9.fit$finalModel)[,300]
#For lambda = 0.1
lm9.fit$finalModel$lambda[227]
coef(lm9.fit$finalModel)[,227]
#######Principal component regression
set.seed(150) #For replication
#With categorical variables, formula method should be used
pcr.fit = train(form = TMIN230 ~ .,
data = fTR,
method = "pcr",
tuneGrid = data.frame(ncomp = 1:(ncol(fTR)-1)),
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
pcr.fit #information about the resampling
ggplot(pcr.fit)
#Variance explained
summary(pcr.fit$finalModel)
pcr.fit$finalModel$Xvar
#plot loadings of component 1
dataplot = data.frame(x = rownames(pcr.fit$finalModel$loadings), y =pcr.fit$finalModel$loadings[,1])
ggplot(dataplot)+ geom_col(aes(x=x,y=y))
#plot loadings of component 1
dataplot = data.frame(x = rownames(pcr.fit$finalModel$loadings), y =pcr.fit$finalModel$loadings[,1])
#plot loadings of component 1
dataplot = data.frame(x = rownames(pcr.fit$finalModel$loadings), y =pcr.fit$finalModel$loadings[,1])
ggplot(dataplot)+ geom_col(aes(x=x,y=y))
#######Partial least squares regression
set.seed(150) #For replication
#With categorical variables, formula method should be used
plsr.fit = train(form = TMIN230 ~ .,
data = fTR,
method = "pls",
tuneGrid = data.frame(ncomp = 1:(ncol(fTR)-1)),
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
ncol(fTR)
ggplot(plsr.fit)+scale_x_log10()
summary(plsr.fit$finalModel)
#-------------------------------------------------------------------------------------------------
#--------------------------- Training results ------------------------------------------
#-------------------------------------------------------------------------------------------------
transformResults <- resamples(list(
lm5.ALL = lm5.fit,
lm6.BYHAND=lm6.fit,
lm7.RFE=lm7.RFE,
lm8.RIDGE=lm8.fit,
lm9.LASSO = lm9.fit,
pcr.fit = pcr.fit,
plsr.fit = plsr.fit))
summary(transformResults)
dotplot(transformResults)
source('C:/Users/Ignacio/OneDrive - Universidad Pontificia Comillas/ICAI/1ยบ de Master/Machine learning/Regression/Practica_3_GAM/LabPractice_3_2_Demad_Solution.R')
## LoadData
fdata <- read.csv("DAILY_DEMAND.csv", sep = ";")
#Plot relation between temperature and demand
ggplot(fdata)+geom_point(aes(x=TEMP, y=DEM))
# Set variable types -----------------------------------------
fdata$WD = as.factor(fdata$WD);
fdata$fecha <- NULL
str(fdata)
## Exploratory analysis -------------------------------------------------------------------------------------
PlotDataframe(fdata, output.name = "DEM")
#correlation plot of numeric variables
numvars <- sapply(fdata, class) %in% c("integer","numeric")
C <- cor(fdata[,numvars])
corrplot(C, method = "number")
## Divide the data into training and validation sets ---------------------------------------------------
set.seed(150) #For replication
ratioTR = 0.8 #Percentage for training
#create random 80/20 % split
trainIndex <- createDataPartition(fdata$DEM,     #output variable. createDataPartition creates proportional partitions
p = ratioTR, #split probability
list = FALSE, #Avoid output as a list
times = 1) #only one partition
#obtain training and validation sets
fTR = fdata[trainIndex,]
fTS = fdata[-trainIndex,]
## Initialize trainControl -----------------------------------------------------------------------
#Use resampling for measuring generalization error
#K-fold with 10 folds
ctrl_tune <- trainControl(method = "cv",
number = 10,
summaryFunction = defaultSummary,    #Performance summary for comparing models in hold-out samples.
returnResamp = "final",              #Return final information about resampling
savePredictions = TRUE)              #save predictions
## Linear Regression -------------------------------------------------------------------------------------------
set.seed(150) #For replication
lm.fit = train(form = DEM ~ WD + TEMP,
data = fTR,
method = "lm", #Linear model
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
lm.fit #information about the resampling settings
summary(lm.fit)  #information about the model trained
#Evaluate the model with training sets and diagnosis
fTR_eval = fTR
fTR_eval$lm_pred = predict(lm.fit,  newdata = fTR)
PlotModelDiagnosis(fTR, fTR$DEM,
fTR_eval$lm_pred,
together = TRUE)
#Relation between DEM and TEMP
ggplot(fTR_eval)+geom_point(aes(x=TEMP, y = DEM))+geom_point(aes(x=TEMP, y = lm_pred,  color=WD))
#-------------------------------------------------------------------------------------------------
#----------------------- polynomial regression  ------------------------------------------------
#-------------------------------------------------------------------------------------------------
set.seed(150) #For replication
poly.fit = train(form = DEM ~ WD + poly(TEMP, degree = 2, raw = TRUE),
data = fTR,
method = "lm", #Linear model
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
poly.fit #information about the resampling settings
summary(poly.fit)  #information about the model trained
#Evaluate the model with training sets and diagnosis
fTR_eval$poly_pred = predict(poly.fit,  newdata = fTR)
PlotModelDiagnosis(fTR, fTR$DEM,
fTR_eval$poly_pred,
together = TRUE)
#Relation between DEM and TEMP
ggplot(fTR_eval)+geom_point(aes(x=TEMP, y = DEM))+geom_point(aes(x=TEMP, y = poly_pred,  color=WD))
set.seed(150) #For replication
gam.fit = train(form = DEM ~ WD + TEMP,
data = fTR,
method = "gamSpline",
tuneGrid = data.frame(df = seq(2,10,2)),
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
gam.fit #information about the resampling settings
ggplot(gam.fit)
summary(gam.fit)  #information about the model trained
#Plot the fitted splines
#Careful, if library "car" is loaded, this plot gives an error
plot(gam.fit$finalModel, se=TRUE ,col ="blue ")
#Evaluate the model with training sets and diagnosis
fTR_eval$gam_pred = predict(gam.fit,  newdata = fTR)
PlotModelDiagnosis(fTR, fTR$DEM,
fTR_eval$gam_pred,
together = TRUE)
#Relation between DEM and TEMP
ggplot(fTR_eval)+geom_point(aes(x=TEMP, y = DEM))+geom_point(aes(x=TEMP, y = gam_pred,  color=WD))
#-------------------------------------------------------------------------------------------------
#----------------------- GAM with splines & Holidays   ------------------------------------------------
#-------------------------------------------------------------------------------------------------
# POSSIBLE IMPROVEMENTS
# - Holidays (How?)
fTR$WD[(fTR_eval$DEM-fTR_eval$gam_pred)<(-80)] <- 7
set.seed(150) #For replication
gam.fit = train(form = DEM ~ WD + TEMP,
data = fTR,
method = "gamSpline",
tuneGrid = data.frame(df = seq(2,10,2)),
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
gam.fit #information about the resampling settings
ggplot(gam.fit)
summary(gam.fit)  #information about the model trained
#Plot the fitted splines
#Careful, if library "car" is loaded, this plot gives an error
plot(gam.fit$finalModel, se=TRUE ,col ="blue ")
#Plot the fitted splines
#Careful, if library "car" is loaded, this plot gives an error
plot(gam.fit$finalModel, se=TRUE ,col ="blue ")
ggplot(gam.fit)
#Plot the fitted splines
#Careful, if library "car" is loaded, this plot gives an error
plot(gam.fit$finalModel, se=TRUE ,col ="blue ")
#Evaluate the model with training sets and diagnosis
fTR_eval$gam_pred = predict(gam.fit,  newdata = fTR)
PlotModelDiagnosis(fTR, fTR$DEM,
fTR_eval$gam_pred,
together = TRUE)
#Relation between DEM and TEMP
ggplot(fTR_eval)+geom_point(aes(x=TEMP, y = DEM))+geom_point(aes(x=TEMP, y = gam_pred,  color=WD))
source('C:/Users/Ignacio/OneDrive - Universidad Pontificia Comillas/ICAI/1ยบ de Master/Machine learning/Regression/Practica_4_MLP/Lab Practica 3_4.R', echo=TRUE)
## Exploratory analysis -------------------------------------------------------------------------------------
ggplot(fdata) + geom_line(aes(x=Time, y=y))
ggpairs(fdata,aes( alpha = 0.01))
## Divide the data into training and validation sets ---------------------------------------------------
set.seed(150) #For replication
ratioTR = 0.8 #Percentage for training
#create random 80/20 % split
trainIndex <- createDataPartition(fdata$y,     #output variable. createDataPartition creates proportional partitions
p = ratioTR, #split probability
list = FALSE, #Avoid output as a list
times = 1) #only one partition
#obtain training and validation sets
fTR = fdata[trainIndex,]
fTS = fdata[-trainIndex,]
#Create index to select input variables
varindex <- variable.names(fdata) != "y"
## Initialize trainControl -----------------------------------------------------------------------
#Use resampling for measuring generalization error
#K-fold with 10 folds
ctrl_tune <- trainControl(method = "cv",
number = 10,
summaryFunction = defaultSummary,    #Performance summary for comparing models in hold-out samples.
returnResamp = "final",              #Return final information about resampling
savePredictions = TRUE)              #save predictions
## Linear Regression -------------------------------------------------------------------------------------------
set.seed(150) #For replication
lm.fit = train(fTR[,varindex],
y = fTR$y,
method = "lm", #Linear model
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
lm.fit #information about the resampling settings
summary(lm.fit)  #information about the model trained
#Evaluate the model with training sets and diagnosis
fTR_eval = fTR
fTR_eval$lm_pred = predict(lm.fit,  newdata = fTR)
fTS_eval = fTS
fTS_eval$lm_pred = predict(lm.fit,  newdata = fTS)
PlotModelDiagnosis(fTR[,varindex], fTR$y,
fTR_eval$lm_pred, together = TRUE)
## MLP -------------------------------------------------------------------------------------------
set.seed(150) #For replication
mlp.fit = train(form = y~.,
data = fTR,
method = "nnet",
linout = TRUE,
maxit = 250,
#tuneGrid = data.frame(size = 10, decay = 0),
tuneGrid = expand.grid(size = seq(5,25,length.out = 5), decay=10^seq(-7,-2, length.out=6)),
#tuneLength = 5,
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
mlp.fit #information about the resampling settings
ggplot(mlp.fit) + scale_x_log10()
fTR_eval$mlp_pred = predict(mlp.fit,  newdata = fTR)
fTS_eval$mlp_pred = predict(mlp.fit,  newdata = fTS)
library(NeuralSens)
SensAnalysisMLP(mlp.fit) #Statistical sensitivity analysis
PlotModelDiagnosis(fTR[,varindex], fTR$y,
fTR_eval$mlp_pred, together = TRUE)
# Input variable selection -----------------------
set.seed(150) #For replication
mlp2.fit = train(form = y~X1+X2+X3+X4,
data = fTR,
method = "nnet",
linout = TRUE,
maxit = 300,
#tuneGrid = data.frame(size =5, decay = 0),
tuneGrid = expand.grid(size = seq(5,25,length.out = 5), decay=10^seq(-7,-2, length.out=6)),                #tuneLength = 5,
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
mlp2.fit #information about the resampling settings
ggplot(mlp2.fit) + scale_x_log10()
library(NeuralSens)
SensAnalysisMLP(mlp2.fit) #Statistical sensitivity analysis
fTR_eval$mlp2_pred = predict(mlp2.fit,  newdata = fTR)
fTS_eval$mlp2_pred = predict(mlp2.fit,  newdata = fTS)
PlotModelDiagnosis(fTR[,c("X1","X2","X3","X4")], fTR$y,
fTR_eval$mlp2_pred, together = TRUE)
ggplot(fTR_eval) + geom_point(aes(x=y, y=lm_pred, color="lm"))+
geom_point(aes(x=y, y=mlp2_pred, color="mlp2"))
ggplot(fTS_eval) + geom_point(aes(x=y, y=lm_pred, color="lm"))+
geom_point(aes(x=y, y=mlp2_pred, color="mlp2"))
# Model comparison
transformResults <- resamples(list(lm=lm.fit, mlp=mlp.fit, mlp2=mlp2.fit))
summary(transformResults)
dotplot(transformResults)
## svm -------------------------------------------------------------------------------------------
library(kernlab)
set.seed(150) #For replication
svm.fit = train(form = y~.,
data = fTR,
method = "svmRadial",
#tuneLength = 5,
#tuneGrid =  data.frame( sigma=10, C=1),
tuneGrid = expand.grid(C = 10^seq(-1,2,length.out = 6), sigma=10^seq(-3,1,length.out=5)),
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
svm.fit #information about the resampling
ggplot(svm.fit) #plot the summary metric as a function of the tuning parameter
# Evaluate training performance -------------------------------------------------------------------------
transformResults <- resamples(list(lm=lm.fit, mlp=mlp.fit, mlp2=mlp2.fit, svm = svm.fit))
summary(transformResults)
dotplot(transformResults)
# Evaluate validation performance -------------------------------------------------------------------------
fTS_eval = fTS
fTS_eval$lm_pred = predict(lm.fit,  newdata = fTS)
fTS_eval$mlp_pred = predict(mlp.fit,  newdata = fTS)
fTS_eval$mlp2_pred = predict(mlp2.fit,  newdata = fTS)
fTS_eval$svm_pred = predict(svm.fit,  newdata = fTS)
caret::R2(fTS_eval$lm_pred,fTS_eval$y)
caret::R2(fTS_eval$mlp_pred,fTS_eval$y)
caret::R2(fTS_eval$mlp2_pred,fTS_eval$y)
caret::R2(fTS_eval$svm_pred,fTS_eval$y)
caret::RMSE(fTS_eval$lm_pred,fTS_eval$y)
caret::RMSE(fTS_eval$mlp_pred,fTS_eval$y)
caret::RMSE(fTS_eval$mlp2_pred,fTS_eval$y)
caret::RMSE(fTS_eval$svm_pred,fTS_eval$y)
ggplot(fTS_eval) + geom_point(aes(x=y, y=lm_pred, color="lm"))+
geom_point(aes(x=y, y=mlp2_pred, color="mlp2"))+
geom_point(aes(x=y, y=svm_pred, color="svm"))
# Input variable selection -----------------------
set.seed(150) #For replication
svm2.fit = train(form = y~X1+X2+X3+X4,
data = fTR,
method = "svmRadial",
#tuneLength = 5,
#tuneGrid =  data.frame( sigma=10, C=1),
tuneGrid = expand.grid(C = 10^seq(-1,2,length.out = 6), sigma=10^seq(-3,1,length.out=5)),
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
svm2.fit #information about the resampling
ggplot(svm2.fit) #plot the summary metric as a function of the tuning parameter
fTS_eval$svm2_pred = predict(svm2.fit,  newdata = fTS)
# Evaluate training performance -------------------------------------------------------------------------
transformResults <- resamples(list(lm=lm.fit, mlp=mlp.fit, mlp2=mlp2.fit, svm = svm.fit, svm2=svm2.fit))
summary(transformResults)
dotplot(transformResults)
caret::R2(fTS_eval$lm_pred,fTS_eval$y)
caret::R2(fTS_eval$mlp_pred,fTS_eval$y)
caret::R2(fTS_eval$mlp2_pred,fTS_eval$y)
caret::R2(fTS_eval$svm_pred,fTS_eval$y)
caret::R2(fTS_eval$svm2_pred,fTS_eval$y)
caret::RMSE(fTS_eval$lm_pred,fTS_eval$y)
caret::RMSE(fTS_eval$mlp_pred,fTS_eval$y)
caret::RMSE(fTS_eval$mlp2_pred,fTS_eval$y)
caret::RMSE(fTS_eval$svm_pred,fTS_eval$y)
caret::RMSE(fTS_eval$svm2_pred,fTS_eval$y)
